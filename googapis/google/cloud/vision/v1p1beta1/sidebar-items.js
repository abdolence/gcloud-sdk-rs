initSidebarItems({"enum":[["Likelihood","A bucketized representation of likelihood, which is intended to give clients highly stable results across model upgrades."]],"mod":[["block","Nested message and enum types in `Block`."],["face_annotation","Nested message and enum types in `FaceAnnotation`."],["feature","Nested message and enum types in `Feature`."],["image_annotator_client","Generated client implementations."],["text_annotation","Nested message and enum types in `TextAnnotation`."],["web_detection","Nested message and enum types in `WebDetection`."]],"struct":[["AnnotateImageRequest","Request for performing Google Cloud Vision API tasks over a user-provided image, with user-requested features."],["AnnotateImageResponse","Response to an image annotation request."],["BatchAnnotateImagesRequest","Multiple image annotation requests are batched into a single service call."],["BatchAnnotateImagesResponse","Response to a batch image annotation request."],["Block","Logical element on the page."],["BoundingPoly","A bounding polygon for the detected image annotation."],["ColorInfo","Color information consists of RGB channels, score, and the fraction of the image that the color occupies in the image."],["CropHint","Single crop hint that is used to generate a new crop when serving an image."],["CropHintsAnnotation","Set of crop hints that are used to generate new crops when serving images."],["CropHintsParams","Parameters for crop hints annotation request."],["DominantColorsAnnotation","Set of dominant colors and their corresponding scores."],["EntityAnnotation","Set of detected entity features."],["FaceAnnotation","A face annotation object contains the results of face detection."],["Feature","Users describe the type of Google Cloud Vision API tasks to perform over images by using Features. Each Feature indicates a type of image detection task to perform. Features encode the Cloud Vision API vertical to operate on and the number of top-scoring results to return."],["Image","Client image to perform Google Cloud Vision API tasks over."],["ImageContext","Image context and/or feature-specific parameters."],["ImageProperties","Stores image properties, such as dominant colors."],["ImageSource","External image source (Google Cloud Storage image location)."],["LatLongRect","Rectangle determined by min and max `LatLng` pairs."],["LocationInfo","Detected entity location information."],["Page","Detected page from OCR."],["Paragraph","Structural unit of text representing a number of words in certain order."],["Position","A 3D position in the image, used primarily for Face detection landmarks. A valid Position must have both x and y coordinates. The position coordinates are in the same scale as the original image."],["Property","A `Property` consists of a user-supplied name/value pair."],["SafeSearchAnnotation","Set of features pertaining to the image, computed by computer vision methods over safe-search verticals (for example, adult, spoof, medical, violence)."],["Symbol","A single symbol representation."],["TextAnnotation","TextAnnotation contains a structured representation of OCR extracted text. The hierarchy of an OCR extracted text structure is like this: TextAnnotation -> Page -> Block -> Paragraph -> Word -> Symbol Each structural component, starting from Page, may further have their own properties. Properties describe detected languages, breaks etc.. Please refer to the [TextAnnotation.TextProperty][google.cloud.vision.v1p1beta1.TextAnnotation.TextProperty] message definition below for more detail."],["TextDetectionParams","Parameters for text detections. This is used to control TEXT_DETECTION and DOCUMENT_TEXT_DETECTION features."],["Vertex","A vertex represents a 2D point in the image. NOTE: the vertex coordinates are in the same scale as the original image."],["WebDetection","Relevant information for the image from the Internet."],["WebDetectionParams","Parameters for web detection request."],["Word","A word representation."]]});