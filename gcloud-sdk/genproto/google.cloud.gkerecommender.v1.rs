// This file is @generated by prost-build.
/// Request message for
/// \[GkeInferenceQuickstart.FetchModels\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModels\].
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct FetchModelsRequest {
    /// Optional. The target number of results to return in a single response.
    /// If not specified, a default value will be chosen by the service.
    /// Note that the response may include a partial list and a caller should
    /// only rely on the response's
    /// \[next_page_token\]\[google.cloud.gkerecommender.v1.FetchModelsResponse.next_page_token\]
    /// to determine if there are more instances left to be queried.
    #[prost(int32, optional, tag = "1")]
    pub page_size: ::core::option::Option<i32>,
    /// Optional. The value of
    /// \[next_page_token\]\[google.cloud.gkerecommender.v1.FetchModelsResponse.next_page_token\]
    /// received from a previous `FetchModelsRequest` call.
    /// Provide this to retrieve the subsequent page in a multi-page list of
    /// results. When paginating, all other parameters provided to
    /// `FetchModelsRequest` must match the call that provided the page token.
    #[prost(string, optional, tag = "2")]
    pub page_token: ::core::option::Option<::prost::alloc::string::String>,
}
/// Response message for
/// \[GkeInferenceQuickstart.FetchModels\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModels\].
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct FetchModelsResponse {
    /// Output only. List of available models. Open-source models follow the
    /// Huggingface Hub `owner/model_name` format.
    #[prost(string, repeated, tag = "1")]
    pub models: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Output only. A token which may be sent as
    /// \[page_token\]\[FetchModelsResponse.page_token\] in a subsequent
    /// `FetchModelsResponse` call to retrieve the next page of results.
    /// If this field is omitted or empty, then there are no more results to
    /// return.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// \[GkeInferenceQuickstart.FetchModelServers\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModelServers\].
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct FetchModelServersRequest {
    /// Required. The model for which to list model servers. Open-source models
    /// follow the Huggingface Hub `owner/model_name` format. Use
    /// \[GkeInferenceQuickstart.FetchModels\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModels\]
    /// to find available models.
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Optional. The target number of results to return in a single response.
    /// If not specified, a default value will be chosen by the service.
    /// Note that the response may include a partial list and a caller should
    /// only rely on the response's
    /// \[next_page_token\]\[google.cloud.gkerecommender.v1.FetchModelServersResponse.next_page_token\]
    /// to determine if there are more instances left to be queried.
    #[prost(int32, optional, tag = "2")]
    pub page_size: ::core::option::Option<i32>,
    /// Optional. The value of
    /// \[next_page_token\]\[google.cloud.gkerecommender.v1.FetchModelServersResponse.next_page_token\]
    /// received from a previous `FetchModelServersRequest` call.
    /// Provide this to retrieve the subsequent page in a multi-page list of
    /// results. When paginating, all other parameters provided to
    /// `FetchModelServersRequest` must match the call that provided the page
    /// token.
    #[prost(string, optional, tag = "3")]
    pub page_token: ::core::option::Option<::prost::alloc::string::String>,
}
/// Response message for
/// \[GkeInferenceQuickstart.FetchModelServers\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModelServers\].
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct FetchModelServersResponse {
    /// Output only. List of available model servers. Open-source model servers use
    /// simplified, lowercase names (e.g., `vllm`).
    #[prost(string, repeated, tag = "1")]
    pub model_servers: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Output only. A token which may be sent as
    /// \[page_token\]\[FetchModelServersResponse.page_token\] in a subsequent
    /// `FetchModelServersResponse` call to retrieve the next page of results.
    /// If this field is omitted or empty, then there are no more results to
    /// return.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// \[GkeInferenceQuickstart.FetchModelServerVersions\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModelServerVersions\].
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct FetchModelServerVersionsRequest {
    /// Required. The model for which to list model server versions. Open-source
    /// models follow the Huggingface Hub `owner/model_name` format. Use
    /// \[GkeInferenceQuickstart.FetchModels\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModels\]
    /// to find available models.
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Required. The model server for which to list versions. Open-source model
    /// servers use simplified, lowercase names (e.g., `vllm`). Use
    /// \[GkeInferenceQuickstart.FetchModelServers\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModelServers\]
    /// to find available model servers.
    #[prost(string, tag = "2")]
    pub model_server: ::prost::alloc::string::String,
    /// Optional. The target number of results to return in a single response.
    /// If not specified, a default value will be chosen by the service.
    /// Note that the response may include a partial list and a caller should
    /// only rely on the response's
    /// \[next_page_token\]\[google.cloud.gkerecommender.v1.FetchModelServerVersionsResponse.next_page_token\]
    /// to determine if there are more instances left to be queried.
    #[prost(int32, optional, tag = "3")]
    pub page_size: ::core::option::Option<i32>,
    /// Optional. The value of
    /// \[next_page_token\]\[google.cloud.gkerecommender.v1.FetchModelServerVersionsResponse.next_page_token\]
    /// received from a previous `FetchModelServerVersionsRequest` call.
    /// Provide this to retrieve the subsequent page in a multi-page list of
    /// results. When paginating, all other parameters provided to
    /// `FetchModelServerVersionsRequest` must match the call that provided the
    /// page token.
    #[prost(string, optional, tag = "4")]
    pub page_token: ::core::option::Option<::prost::alloc::string::String>,
}
/// Response message for
/// \[GkeInferenceQuickstart.FetchModelServerVersions\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModelServerVersions\].
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct FetchModelServerVersionsResponse {
    /// Output only. A list of available model server versions.
    #[prost(string, repeated, tag = "1")]
    pub model_server_versions: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Output only. A token which may be sent as
    /// \[page_token\]\[FetchModelServerVersionsResponse.page_token\] in a subsequent
    /// `FetchModelServerVersionsResponse` call to retrieve the next page of
    /// results. If this field is omitted or empty, then there are no more results
    /// to return.
    #[prost(string, tag = "2")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Request message for
/// \[GkeInferenceQuickstart.FetchBenchmarkingData\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchBenchmarkingData\].
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct FetchBenchmarkingDataRequest {
    /// Required. The model server configuration to get benchmarking data for. Use
    /// \[GkeInferenceQuickstart.FetchProfiles\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchProfiles\]
    /// to find valid configurations.
    #[prost(message, optional, tag = "1")]
    pub model_server_info: ::core::option::Option<ModelServerInfo>,
    /// Optional. The instance type to filter benchmarking data. Instance types are
    /// in the format `a2-highgpu-1g`. If not provided, all instance types for the
    /// given profile's `model_server_info` will be returned. Use
    /// \[GkeInferenceQuickstart.FetchProfiles\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchProfiles\]
    /// to find available instance types.
    #[prost(string, tag = "3")]
    pub instance_type: ::prost::alloc::string::String,
    /// Optional. The pricing model to use for the benchmarking data. Defaults to
    /// `spot`.
    #[prost(string, tag = "4")]
    pub pricing_model: ::prost::alloc::string::String,
}
/// Response message for
/// \[GkeInferenceQuickstart.FetchBenchmarkingData\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchBenchmarkingData\].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FetchBenchmarkingDataResponse {
    /// Output only. List of profiles containing their respective benchmarking
    /// data.
    #[prost(message, repeated, tag = "1")]
    pub profile: ::prost::alloc::vec::Vec<Profile>,
}
/// Request message for
/// \[GkeInferenceQuickstart.FetchProfiles\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchProfiles\].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FetchProfilesRequest {
    /// Optional. The model to filter profiles by. Open-source models follow the
    /// Huggingface Hub `owner/model_name` format. If not provided, all models are
    /// returned. Use
    /// \[GkeInferenceQuickstart.FetchModels\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModels\]
    /// to find available models.
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Optional. The model server to filter profiles by. If not provided, all
    /// model servers are returned. Use
    /// \[GkeInferenceQuickstart.FetchModelServers\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModelServers\]
    /// to find available model servers for a given model.
    #[prost(string, tag = "2")]
    pub model_server: ::prost::alloc::string::String,
    /// Optional. The model server version to filter profiles by. If not provided,
    /// all model server versions are returned. Use
    /// \[GkeInferenceQuickstart.FetchModelServerVersions\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModelServerVersions\]
    /// to find available versions for a given model and server.
    #[prost(string, tag = "3")]
    pub model_server_version: ::prost::alloc::string::String,
    /// Optional. The performance requirements to filter profiles. Profiles that do
    /// not meet these requirements are filtered out. If not provided, all profiles
    /// are returned.
    #[prost(message, optional, tag = "4")]
    pub performance_requirements: ::core::option::Option<PerformanceRequirements>,
    /// Optional. The target number of results to return in a single response. If
    /// not specified, a default value will be chosen by the service. Note that the
    /// response may include a partial list and a caller should only rely on the
    /// response's
    /// \[next_page_token\]\[google.cloud.gkerecommender.v1.FetchProfilesResponse.next_page_token\]
    /// to determine if there are more instances left to be queried.
    #[prost(int32, optional, tag = "5")]
    pub page_size: ::core::option::Option<i32>,
    /// Optional. The value of
    /// \[next_page_token\]\[google.cloud.gkerecommender.v1.FetchProfilesResponse.next_page_token\]
    /// received from a previous `FetchProfilesRequest` call.
    /// Provide this to retrieve the subsequent page in a multi-page list of
    /// results. When paginating, all other parameters provided to
    /// `FetchProfilesRequest` must match the call that provided the page
    /// token.
    #[prost(string, optional, tag = "6")]
    pub page_token: ::core::option::Option<::prost::alloc::string::String>,
}
/// Performance requirements for a profile and or model deployment.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PerformanceRequirements {
    /// Optional. The target Normalized Time Per Output Token (NTPOT) in
    /// milliseconds. NTPOT is calculated as `request_latency /  total_output_tokens`. If not provided, this target will not be enforced.
    #[prost(int32, optional, tag = "1")]
    pub target_ntpot_milliseconds: ::core::option::Option<i32>,
    /// Optional. The target Time To First Token (TTFT) in milliseconds. TTFT is
    /// the time it takes to generate the first token for a request.  If not
    /// provided, this target will not be enforced.
    #[prost(int32, optional, tag = "2")]
    pub target_ttft_milliseconds: ::core::option::Option<i32>,
    /// Optional. The target cost for running a profile's model server. If not
    /// provided, this requirement will not be enforced.
    #[prost(message, optional, tag = "3")]
    pub target_cost: ::core::option::Option<Cost>,
}
/// Represents an amount of money in a specific currency.
#[derive(Clone, Copy, PartialEq, Eq, Hash, ::prost::Message)]
pub struct Amount {
    /// Output only. The whole units of the amount.
    /// For example if `currencyCode` is `"USD"`, then 1 unit is one US dollar.
    #[prost(int64, tag = "1")]
    pub units: i64,
    /// Output only. Number of nano (10^-9) units of the amount.
    /// The value must be between -999,999,999 and +999,999,999 inclusive.
    /// If `units` is positive, `nanos` must be positive or zero.
    /// If `units` is zero, `nanos` can be positive, zero, or negative.
    /// If `units` is negative, `nanos` must be negative or zero.
    /// For example $-1.75 is represented as `units`=-1 and `nanos`=-750,000,000.
    #[prost(int32, tag = "2")]
    pub nanos: i32,
}
/// Cost for running a model deployment on a given instance type. Currently, only
/// USD currency code is supported.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Cost {
    /// Optional. The cost per million output tokens, calculated as:
    /// $/output token = GPU $/s / (1/output-to-input-cost-ratio * input tokens/s +
    /// output tokens/s)
    #[prost(message, optional, tag = "1")]
    pub cost_per_million_output_tokens: ::core::option::Option<Amount>,
    /// Optional. The cost per million input tokens. $/input token = ($/output
    /// token) / output-to-input-cost-ratio.
    #[prost(message, optional, tag = "2")]
    pub cost_per_million_input_tokens: ::core::option::Option<Amount>,
    /// Optional. The pricing model used to calculate the cost. Can be one of:
    /// `3-years-cud`, `1-year-cud`, `on-demand`, `spot`. If not provided, `spot`
    /// will be used.
    #[prost(string, tag = "3")]
    pub pricing_model: ::prost::alloc::string::String,
    /// Optional. The output-to-input cost ratio. This determines how the total GPU
    /// cost is split between input and output tokens. If not provided, `4.0` is
    /// used, assuming a 4:1 output:input cost ratio.
    #[prost(float, optional, tag = "4")]
    pub output_input_cost_ratio: ::core::option::Option<f32>,
}
/// Represents a range of throughput values in tokens per second.
#[derive(Clone, Copy, PartialEq, Eq, Hash, ::prost::Message)]
pub struct TokensPerSecondRange {
    /// Output only. The minimum value of the range.
    #[prost(int32, tag = "1")]
    pub min: i32,
    /// Output only. The maximum value of the range.
    #[prost(int32, tag = "2")]
    pub max: i32,
}
/// Represents a range of latency values in milliseconds.
#[derive(Clone, Copy, PartialEq, Eq, Hash, ::prost::Message)]
pub struct MillisecondRange {
    /// Output only. The minimum value of the range.
    #[prost(int32, tag = "1")]
    pub min: i32,
    /// Output only. The maximum value of the range.
    #[prost(int32, tag = "2")]
    pub max: i32,
}
/// Performance range for a model deployment.
#[derive(Clone, Copy, PartialEq, Eq, Hash, ::prost::Message)]
pub struct PerformanceRange {
    /// Output only. The range of throughput in output tokens per second. This is
    /// measured as total_output_tokens_generated_by_server /
    /// elapsed_time_in_seconds.
    #[prost(message, optional, tag = "1")]
    pub throughput_output_range: ::core::option::Option<TokensPerSecondRange>,
    /// Output only. The range of TTFT (Time To First Token) in milliseconds. TTFT
    /// is the time it takes to generate the first token for a request.
    #[prost(message, optional, tag = "2")]
    pub ttft_range: ::core::option::Option<MillisecondRange>,
    /// Output only. The range of NTPOT (Normalized Time Per Output Token) in
    /// milliseconds. NTPOT is the request latency normalized by the number of
    /// output tokens, measured as request_latency / total_output_tokens.
    #[prost(message, optional, tag = "3")]
    pub ntpot_range: ::core::option::Option<MillisecondRange>,
}
/// Response message for
/// \[GkeInferenceQuickstart.FetchProfiles\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchProfiles\].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct FetchProfilesResponse {
    /// Output only. List of profiles that match the given model server info and
    /// performance requirements (if provided).
    #[prost(message, repeated, tag = "1")]
    pub profile: ::prost::alloc::vec::Vec<Profile>,
    /// Output only. The combined range of performance values observed across all
    /// profiles in this response.
    #[prost(message, optional, tag = "2")]
    pub performance_range: ::core::option::Option<PerformanceRange>,
    /// Output only. Additional comments related to the response.
    #[prost(string, tag = "3")]
    pub comments: ::prost::alloc::string::String,
    /// Output only. A token which may be sent as
    /// \[page_token\]\[FetchProfilesResponse.page_token\] in a subsequent
    /// `FetchProfilesResponse` call to retrieve the next page of results. If this
    /// field is omitted or empty, then there are no more results to return.
    #[prost(string, tag = "4")]
    pub next_page_token: ::prost::alloc::string::String,
}
/// Model server information gives. Valid model server info combinations can
/// be found using
/// \[GkeInferenceQuickstart.FetchProfiles\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchProfiles\].
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct ModelServerInfo {
    /// Required. The model. Open-source models follow the Huggingface Hub
    /// `owner/model_name` format. Use
    /// \[GkeInferenceQuickstart.FetchModels\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModels\]
    /// to find available models.
    #[prost(string, tag = "1")]
    pub model: ::prost::alloc::string::String,
    /// Required. The model server. Open-source model servers use simplified,
    /// lowercase names (e.g., `vllm`). Use
    /// \[GkeInferenceQuickstart.FetchModelServers\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModelServers\]
    /// to find available servers.
    #[prost(string, tag = "2")]
    pub model_server: ::prost::alloc::string::String,
    /// Optional. The model server version. Use
    /// \[GkeInferenceQuickstart.FetchModelServerVersions\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchModelServerVersions\]
    /// to find available versions. If not provided, the latest available version
    /// is used.
    #[prost(string, tag = "3")]
    pub model_server_version: ::prost::alloc::string::String,
}
/// Resources used by a model deployment.
#[derive(Clone, Copy, PartialEq, Eq, Hash, ::prost::Message)]
pub struct ResourcesUsed {
    /// Output only. The number of accelerators (e.g., GPUs or TPUs) used by the
    /// model deployment on the Kubernetes node.
    #[prost(int32, tag = "1")]
    pub accelerator_count: i32,
}
/// Performance statistics for a model deployment.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct PerformanceStats {
    /// Output only. The number of queries per second.
    /// Note: This metric can vary widely based on context length and may not be a
    /// reliable measure of LLM throughput.
    #[prost(float, tag = "1")]
    pub queries_per_second: f32,
    /// Output only. The number of output tokens per second. This is the throughput
    /// measured as total_output_tokens_generated_by_server /
    /// elapsed_time_in_seconds.
    #[prost(int32, tag = "2")]
    pub output_tokens_per_second: i32,
    /// Output only. The Normalized Time Per Output Token (NTPOT) in milliseconds.
    /// This is the request latency normalized by the number of output tokens,
    /// measured as request_latency / total_output_tokens.
    #[prost(int32, tag = "3")]
    pub ntpot_milliseconds: i32,
    /// Output only. The Time To First Token (TTFT) in milliseconds. This is the
    /// time it takes to generate the first token for a request.
    #[prost(int32, tag = "4")]
    pub ttft_milliseconds: i32,
    /// Output only. The cost of running the model deployment.
    #[prost(message, repeated, tag = "5")]
    pub cost: ::prost::alloc::vec::Vec<Cost>,
}
/// A profile containing information about a model deployment.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct Profile {
    /// Output only. The model server configuration. Use
    /// \[GkeInferenceQuickstart.FetchProfiles\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchProfiles\]
    /// to find valid configurations.
    #[prost(message, optional, tag = "1")]
    pub model_server_info: ::core::option::Option<ModelServerInfo>,
    /// Output only. The accelerator type. Expected format: `nvidia-h100-80gb`.
    #[prost(string, tag = "2")]
    pub accelerator_type: ::prost::alloc::string::String,
    /// Output only. The TPU topology (if applicable).
    #[prost(string, tag = "3")]
    pub tpu_topology: ::prost::alloc::string::String,
    /// Output only. The instance type. Expected format: `a2-highgpu-1g`.
    #[prost(string, tag = "4")]
    pub instance_type: ::prost::alloc::string::String,
    /// Output only. The resources used by the model deployment.
    #[prost(message, optional, tag = "5")]
    pub resources_used: ::core::option::Option<ResourcesUsed>,
    /// Output only. The performance statistics for this profile.
    #[prost(message, repeated, tag = "6")]
    pub performance_stats: ::prost::alloc::vec::Vec<PerformanceStats>,
}
/// Request message for
/// \[GkeInferenceQuickstart.GenerateOptimizedManifest\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.GenerateOptimizedManifest\].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateOptimizedManifestRequest {
    /// Required. The model server configuration to generate the manifest for. Use
    /// \[GkeInferenceQuickstart.FetchProfiles\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchProfiles\]
    /// to find valid configurations.
    #[prost(message, optional, tag = "1")]
    pub model_server_info: ::core::option::Option<ModelServerInfo>,
    /// Required. The accelerator type. Use
    /// \[GkeInferenceQuickstart.FetchProfiles\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.FetchProfiles\]
    /// to find valid accelerators for a given `model_server_info`.
    #[prost(string, tag = "2")]
    pub accelerator_type: ::prost::alloc::string::String,
    /// Optional. The kubernetes namespace to deploy the manifests in.
    #[prost(string, tag = "3")]
    pub kubernetes_namespace: ::prost::alloc::string::String,
    /// Optional. The performance requirements to use for generating Horizontal Pod
    /// Autoscaler (HPA) resources. If provided, the manifest includes HPA
    /// resources to adjust the model server replica count to maintain the
    /// specified targets (e.g., NTPOT, TTFT) at a P50 latency. Cost targets are
    /// not currently supported for HPA generation. If the specified targets are
    /// not achievable, the HPA manifest will not be generated.
    #[prost(message, optional, tag = "4")]
    pub performance_requirements: ::core::option::Option<PerformanceRequirements>,
    /// Optional. The storage configuration for the model. If not provided, the
    /// model is loaded from Huggingface.
    #[prost(message, optional, tag = "5")]
    pub storage_config: ::core::option::Option<StorageConfig>,
}
/// A Kubernetes manifest.
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct KubernetesManifest {
    /// Output only. Kubernetes resource kind.
    #[prost(string, tag = "1")]
    pub kind: ::prost::alloc::string::String,
    /// Output only. Kubernetes API version.
    #[prost(string, tag = "2")]
    pub api_version: ::prost::alloc::string::String,
    /// Output only. YAML content.
    #[prost(string, tag = "3")]
    pub content: ::prost::alloc::string::String,
}
/// Response message for
/// \[GkeInferenceQuickstart.GenerateOptimizedManifest\]\[google.cloud.gkerecommender.v1.GkeInferenceQuickstart.GenerateOptimizedManifest\].
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct GenerateOptimizedManifestResponse {
    /// Output only. A list of generated Kubernetes manifests.
    #[prost(message, repeated, tag = "1")]
    pub kubernetes_manifests: ::prost::alloc::vec::Vec<KubernetesManifest>,
    /// Output only. Comments related to deploying the generated manifests.
    #[prost(string, repeated, tag = "2")]
    pub comments: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    /// Output only. Additional information about the versioned dependencies used
    /// to generate the manifests. See [Run best practice inference with GKE
    /// Inference Quickstart
    /// recipes](<https://cloud.google.com/kubernetes-engine/docs/how-to/machine-learning/inference/inference-quickstart>)
    /// for details.
    #[prost(string, tag = "3")]
    pub manifest_version: ::prost::alloc::string::String,
}
/// Storage configuration for a model deployment.
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct StorageConfig {
    /// Optional. The Google Cloud Storage bucket URI to load the model from. This
    /// URI must point to the directory containing the model's config file
    /// (`config.json`) and model weights. A tuned GCSFuse setup can improve
    /// LLM Pod startup time by more than 7x. Expected format:
    /// `gs://<bucket-name>/<path-to-model>`.
    #[prost(string, tag = "1")]
    pub model_bucket_uri: ::prost::alloc::string::String,
    /// Optional. The URI for the GCS bucket containing the XLA compilation cache.
    /// If using TPUs, the XLA cache will be written to the same path as
    /// `model_bucket_uri`. This can speed up vLLM model preparation for repeated
    /// deployments.
    #[prost(string, tag = "2")]
    pub xla_cache_bucket_uri: ::prost::alloc::string::String,
}
/// Generated client implementations.
pub mod gke_inference_quickstart_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    /// GKE Inference Quickstart (GIQ) service provides profiles with performance
    /// metrics for popular models and model servers across multiple accelerators.
    /// These profiles help generate optimized best practices for running inference
    /// on GKE.
    #[derive(Debug, Clone)]
    pub struct GkeInferenceQuickstartClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl GkeInferenceQuickstartClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> GkeInferenceQuickstartClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::Body>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> GkeInferenceQuickstartClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::Body>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::Body>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::Body>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            GkeInferenceQuickstartClient::new(
                InterceptedService::new(inner, interceptor),
            )
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        /// Fetches available models. Open-source models follow the Huggingface Hub
        /// `owner/model_name` format.
        pub async fn fetch_models(
            &mut self,
            request: impl tonic::IntoRequest<super::FetchModelsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::FetchModelsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic_prost::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.gkerecommender.v1.GkeInferenceQuickstart/FetchModels",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.gkerecommender.v1.GkeInferenceQuickstart",
                        "FetchModels",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Fetches available model servers. Open-source model servers use simplified,
        /// lowercase names (e.g., `vllm`).
        pub async fn fetch_model_servers(
            &mut self,
            request: impl tonic::IntoRequest<super::FetchModelServersRequest>,
        ) -> std::result::Result<
            tonic::Response<super::FetchModelServersResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic_prost::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.gkerecommender.v1.GkeInferenceQuickstart/FetchModelServers",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.gkerecommender.v1.GkeInferenceQuickstart",
                        "FetchModelServers",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Fetches available model server versions. Open-source servers use their own
        /// versioning schemas (e.g., `vllm` uses semver like `v1.0.0`).
        ///
        /// Some model servers have different versioning schemas depending on the
        /// accelerator. For example, `vllm` uses semver on GPUs, but returns nightly
        /// build tags on TPUs. All available versions will be returned when different
        /// schemas are present.
        pub async fn fetch_model_server_versions(
            &mut self,
            request: impl tonic::IntoRequest<super::FetchModelServerVersionsRequest>,
        ) -> std::result::Result<
            tonic::Response<super::FetchModelServerVersionsResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic_prost::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.gkerecommender.v1.GkeInferenceQuickstart/FetchModelServerVersions",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.gkerecommender.v1.GkeInferenceQuickstart",
                        "FetchModelServerVersions",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Fetches available profiles. A profile contains performance metrics and
        /// cost information for a specific model server setup. Profiles can be
        /// filtered by parameters. If no filters are provided, all profiles are
        /// returned.
        ///
        /// Profiles display a single value per performance metric based on the
        /// provided performance requirements. If no requirements are given, the
        /// metrics represent the inflection point. See [Run best practice inference
        /// with GKE Inference Quickstart
        /// recipes](https://cloud.google.com/kubernetes-engine/docs/how-to/machine-learning/inference/inference-quickstart#how)
        /// for details.
        pub async fn fetch_profiles(
            &mut self,
            request: impl tonic::IntoRequest<super::FetchProfilesRequest>,
        ) -> std::result::Result<
            tonic::Response<super::FetchProfilesResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic_prost::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.gkerecommender.v1.GkeInferenceQuickstart/FetchProfiles",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.gkerecommender.v1.GkeInferenceQuickstart",
                        "FetchProfiles",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Generates an optimized deployment manifest for a given model and model
        /// server, based on the specified accelerator, performance targets, and
        /// configurations. See [Run best practice inference with GKE Inference
        /// Quickstart
        /// recipes](https://cloud.google.com/kubernetes-engine/docs/how-to/machine-learning/inference/inference-quickstart)
        /// for deployment details.
        pub async fn generate_optimized_manifest(
            &mut self,
            request: impl tonic::IntoRequest<super::GenerateOptimizedManifestRequest>,
        ) -> std::result::Result<
            tonic::Response<super::GenerateOptimizedManifestResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic_prost::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.gkerecommender.v1.GkeInferenceQuickstart/GenerateOptimizedManifest",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.gkerecommender.v1.GkeInferenceQuickstart",
                        "GenerateOptimizedManifest",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
        /// Fetches all of the benchmarking data available for a profile. Benchmarking
        /// data returns all of the performance metrics available for a given model
        /// server setup on a given instance type.
        pub async fn fetch_benchmarking_data(
            &mut self,
            request: impl tonic::IntoRequest<super::FetchBenchmarkingDataRequest>,
        ) -> std::result::Result<
            tonic::Response<super::FetchBenchmarkingDataResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic_prost::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/google.cloud.gkerecommender.v1.GkeInferenceQuickstart/FetchBenchmarkingData",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new(
                        "google.cloud.gkerecommender.v1.GkeInferenceQuickstart",
                        "FetchBenchmarkingData",
                    ),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
